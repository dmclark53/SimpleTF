{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "sns.set_context(\"notebook\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn.python.ops import core_rnn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Create a dataset of sine waves. Create two groups, one with one period and another with a different period. Randomly assign different amplitudes to each wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_wave_sample(sample_size, num_points, period, print_stats=False):\n",
    "    \"\"\"\n",
    "    Create a random sample of wave patterns given a sample size, number of points, and a period.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    sample_size: int\n",
    "        Number of wave patterns to create.\n",
    "    num_points: int\n",
    "        How many points to use of each wave pattern.\n",
    "    period: float\n",
    "        The wave period to use.\n",
    "    print_stats: bool\n",
    "        Whether to print statistics and plot for first wave sample. Used for debugging.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "    \n",
    "    \"\"\"\n",
    "    wave_list = []\n",
    "    count = 1\n",
    "    for _ in range(sample_size):\n",
    "        a = np.random.uniform(low=-10, high=10.1)\n",
    "        x = np.linspace(-10*np.pi, 10*np.pi, num_points)\n",
    "        wave = a*np.sin(period*x)\n",
    "        wave_and_period = np.insert(float(period), 1, wave)\n",
    "        wave_list.append(wave_and_period)\n",
    "        if print_stats:\n",
    "            print('period: {}'.format(period))\n",
    "            print('sample size: {}'.format(sample_size))\n",
    "            print('number of points: {}'.format(num_points))\n",
    "            print('a: {}'.format(a))\n",
    "            print('len(x): {}'.format(len(x)))\n",
    "            print('x range: [{}, {}]'.format(min(x), max(x)))\n",
    "            print('len(wave_and_period): {}'.format(len(wave_and_period)))\n",
    "            print('wave range: [{}, {}]'.format(min(wave_and_period), max(wave_and_period)))\n",
    "            plt.plot(x, wave)\n",
    "            plt.show()\n",
    "            print_stats=False\n",
    "        count += 1\n",
    "        \n",
    "    return np.array(wave_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate two wave samples with different periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "waves_1 = create_wave_sample(150, 301, 1)\n",
    "waves_2 = create_wave_sample(150, 301, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate both wave samples into a single sample and load into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "waves_sample = np.concatenate([waves_1, waves_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(waves_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300 entries, 0 to 299\n",
      "Columns: 302 entries, 0 to 301\n",
      "dtypes: float64(302)\n",
      "memory usage: 707.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomize the two wave samples by shuffling the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_shuffled = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "* Split dataset into training set and test/validation set.\n",
    "* Split test/validation set into test set and cross validation set.\n",
    "* Create function to take random samples from the training set to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_array = df.ix[:, 1:].as_matrix()\n",
    "y_series = df.ix[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast y labels from periods to values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originals y labels: [ 1.  2.]\n",
      "New y labels: [ 1.  0.]\n"
     ]
    }
   ],
   "source": [
    "unique_ys = y_series.unique()\n",
    "sorted_ys = np.sort(unique_ys)[::-1]\n",
    "for i in range(len(sorted_ys)):\n",
    "    y_series = y_series.replace(to_replace=sorted_ys[i], value=i)\n",
    "new_unique_ys = y_series.unique()\n",
    "y_array = y_series.values\n",
    "print('Originals y labels: {}'.format(unique_ys))\n",
    "print('New y labels: {}'.format(new_unique_ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split features and labels into train and test/validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X_array, y_array, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split test/validation set into test set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_val_size = int(X_test_val.shape[0] / 2)\n",
    "X_test = X_test_val[0:test_val_size, :]\n",
    "y_test = y_test_val[0:test_val_size]\n",
    "X_val = X_test_val[test_val_size:, :]\n",
    "y_val = y_test_val[test_val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to take random samples of batch size from the training data. This type of training is called bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batch(X, y, batch_size, back_prop_steps):\n",
    "    \"\"\"\n",
    "    Create a random batch from the training data.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    X: numpy.ndarray\n",
    "        Training set.\n",
    "    y: numpy.ndarray\n",
    "        Test set.\n",
    "    batch_size: int\n",
    "        Batch size.\n",
    "    back_prop_stesp: int\n",
    "        Back propagation size.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    numpy.ndarray, numpy.ndarray\n",
    "    \n",
    "    \"\"\"\n",
    "    num_rows, num_columns = X.shape\n",
    "    bp_start_i = np.random.choice(num_columns-back_prop_steps, 1)[0]\n",
    "    bp_end_i = bp_start_i + back_prop_steps\n",
    "    random_indices = np.random.choice(np.arange(num_rows), batch_size, replace=False)\n",
    "    X_batch = X[random_indices, bp_start_i:bp_end_i]\n",
    "    y_batch = y[random_indices]\n",
    "    \n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "* Define placeholders\n",
    "* Define hyperparameters\n",
    "    - \\# of epochs\n",
    "    - learning rate\n",
    "    - backprop size\n",
    "    - \\# of LSTM cells\n",
    "* Create LSTM cell\n",
    "* Create softmax layer for classification\n",
    "* Define cost function\n",
    "* Define training step\n",
    "* Create function to check model accuracy\n",
    "* Set up TF session and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Define hyperparameters\n",
    "Create a dictionary to store the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset graph is session isn't closed\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'num_classes': len(unique_ys),\n",
    "    'batch_size': X_train.shape[0] // 10,\n",
    "    'back_prop_steps': 50,\n",
    "    'lstm_layers': 2,\n",
    "    'lstm_cell_units': 5,\n",
    "    'drop_out': 0.9,\n",
    "    'clipping_ratio': 5,\n",
    "    'learning_rate': 1e-4,\n",
    "    'epochs': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test and Validation Sets\n",
    "Create test and validation dataset of size `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val, y_val = make_batch(X_val, y_val, hyperparams['batch_size'], hyperparams['back_prop_steps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_tensor = tf.placeholder(tf.float32, [None, hyperparams['back_prop_steps']], name='Features')\n",
    "y_tensor = tf.placeholder(tf.int64, [None], name='Labels')\n",
    "keep_prob = tf.placeholder('float', name='Drop_Out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(num_units):\n",
    "    cell = tf.contrib.rnn.LSTMCell(num_units, state_is_tuple=True)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def include_dropout(cell, prob):\n",
    "    cell_culled = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=prob)\n",
    "    return cell_culled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lstm_cell(num_units, prob, num_layers, batch_size, x):\n",
    "#     cell = tf.nn.rnn_cell.LSTMCell(num_units, state_is_tuple=True)\n",
    "#     cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=prob)\n",
    "#    cell = tf.nn.rnn_cell.MultiRNNCell([cell]*num_layers, state_is_tuple=True)\n",
    "    cell_list = [include_dropout(lstm_cell(num_units), prob) for _ in range(num_layers)]\n",
    "    multi_cell = tf.contrib.rnn.MultiRNNCell(cell_list, state_is_tuple=True)\n",
    "    multi_cell_culled = tf.contrib.rnn.DropoutWrapper(multi_cell, output_keep_prob=prob)\n",
    "    initial_state = multi_cell_culled.zero_state(batch_size, tf.float32)\n",
    "    # cell_inputs = tf.expand_dims(x, 2)\n",
    "    return multi_cell_culled  # , cell_inputs, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def set_lstm_state(cell, cell_inputs, initial_state, back_prop_steps):\n",
    "#     cell_outputs = []\n",
    "#     with tf.variable_scope('LSTM_state'):\n",
    "#         for step in range(int(back_prop_steps)):\n",
    "#             if step > 0:\n",
    "#                 tf.get_variable_scope().reuse_variables()\n",
    "#             (cell_output, state) = cell(cell_inputs[:, step, :], initial_state)\n",
    "#             cell_outputs.append(cell_output)\n",
    "#         output = tf.reduce_mean(tf.pack(cell_outputs), 0)\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_rnn(x, cell):\n",
    "    x_list = tf.unstack(tf.expand_dims(x, axis=2), axis=1)\n",
    "    outputs, _ = core_rnn.static_rnn(cell, x_list, dtype=tf.float32)\n",
    "    output = outputs[-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_layer(num_units, num_classes, output):\n",
    "    W = tf.Variable(tf.truncated_normal([num_units, num_classes], stddev=1.0),\n",
    "                   name='weights', dtype=tf.float32, trainable=True)\n",
    "    b = tf.Variable(tf.zeros([num_classes]),\n",
    "                   name='biases', dtype=tf.float32, trainable=True)\n",
    "    output_X_W = tf.matmul(output, W, name='multiply')\n",
    "    logits = tf.add(output_X_W, b, name='add')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_cost_function(logits, y, batch_size):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y, name='cross_entropy')\n",
    "    cost = tf.reduce_sum(loss) / batch_size\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(cost, learning_rate, clipping_ratio):\n",
    "    training_variables = tf.trainable_variables()\n",
    "    computed_gradients = tf.gradients(cost, training_variables)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(computed_gradients, clipping_ratio)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    # gradients = list(zip(clipped_gradients, training_variables))\n",
    "    gradients = list(zip(clipped_gradients, training_variables))\n",
    "    training_step = optimizer.apply_gradients(gradients)\n",
    "    return training_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_accuracy(logits, y):\n",
    "    prediction = tf.equal(tf.argmax(logits, 1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction, 'float'), name='accuracy')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the final model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_final_accuracy(X, y, batch_size, back_prop_steps):\n",
    "    num_rows = X.shape[0]\n",
    "    print('num_rows = {}'.format(num_rows))\n",
    "    num_batches = num_rows // batch_size\n",
    "    print('num_batches = {}'.format(num_batches))\n",
    "    test_accuracy = []\n",
    "    for i in range(int(num_batches)):\n",
    "        X_batch, y_batch = make_batch(X, y, batch_size, back_prop_steps)\n",
    "        result = sess.run([accuracy],\n",
    "                          feed_dict = {X_tensor: X_batch,\n",
    "                                       y_tensor: y_batch,\n",
    "                                       keep_prob: 1})\n",
    "        print('{}: accuracy = {}'.format(i, result))\n",
    "        test_accuracy.append(result)\n",
    "    return np.mean(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create LSTM cell\n",
    "with tf.name_scope(\"LSTM_cell\") as scope:\n",
    "    cell = create_lstm_cell(hyperparams['lstm_cell_units'],\n",
    "                            keep_prob,\n",
    "                            hyperparams['lstm_layers'],\n",
    "                            hyperparams['batch_size'],\n",
    "                            X_tensor)\n",
    "    \n",
    "# # Set LSTM state\n",
    "# with tf.name_scope(\"LSTM_state\") as scope:\n",
    "#     output = set_lstm_state(cell, cell_inputs, initial_state, hyperparams['back_prop_steps'])\n",
    "\n",
    "# Create RNN\n",
    "with tf.name_scope(\"RNN\") as scope:\n",
    "    output = create_rnn(X_tensor, cell)\n",
    "\n",
    "# Create softmax layer\n",
    "with tf.name_scope(\"softmax_layer\") as scope:\n",
    "    logits = softmax_layer(hyperparams['lstm_cell_units'],\n",
    "                          hyperparams['num_classes'],\n",
    "                          output)\n",
    "    \n",
    "# Define cost function\n",
    "with tf.name_scope(\"cost_function\") as scope:\n",
    "    cost = define_cost_function(logits, y_tensor, hyperparams['batch_size'])\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    \n",
    "# Define training step\n",
    "with tf.name_scope(\"training\") as scope:\n",
    "    training_step = train(cost, hyperparams['learning_rate'], hyperparams['clipping_ratio'])\n",
    "    \n",
    "# Calculate cross validation accuracy\n",
    "with tf.name_scope(\"accuracy\") as scope:\n",
    "    accuracy = cross_val_accuracy(logits, y_tensor)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 1000. Cost: [0.693, 0.693], CV Cost: 0.742, CV Accuracy: 0.417\n",
      "100 of 1000. Cost: [0.732, 0.708], CV Cost: 0.725, CV Accuracy: 0.417\n",
      "200 of 1000. Cost: [0.705, 0.704], CV Cost: 0.713, CV Accuracy: 0.417\n",
      "300 of 1000. Cost: [0.715, 0.700], CV Cost: 0.702, CV Accuracy: 0.417\n",
      "400 of 1000. Cost: [0.684, 0.692], CV Cost: 0.688, CV Accuracy: 0.500\n",
      "500 of 1000. Cost: [0.675, 0.683], CV Cost: 0.670, CV Accuracy: 0.667\n",
      "600 of 1000. Cost: [0.664, 0.667], CV Cost: 0.648, CV Accuracy: 0.667\n",
      "700 of 1000. Cost: [0.577, 0.644], CV Cost: 0.593, CV Accuracy: 0.667\n",
      "800 of 1000. Cost: [0.498, 0.597], CV Cost: 0.489, CV Accuracy: 0.917\n",
      "900 of 1000. Cost: [0.389, 0.511], CV Cost: 0.378, CV Accuracy: 0.958\n",
      "num_rows = 30\n",
      "num_batches = 1\n",
      "0: accuracy = [1.0]\n",
      "The final accuracy is 1.000000.\n"
     ]
    }
   ],
   "source": [
    "# Merge summaries for TensorBoard\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "# Start a TensorFlow session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    log_directory = 'tmp/logs'\n",
    "    summary_writer = tf.summary.FileWriter(log_directory, sess.graph)\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    step = 0\n",
    "    cost_train_ma = -np.log(1/float(hyperparams['num_classes']) + 1e-9)\n",
    "    for i in range(hyperparams['epochs']):\n",
    "        # Sample batch for training\n",
    "        X_batch, y_batch = make_batch(X_train, y_train, \n",
    "                                      hyperparams['batch_size'], hyperparams['back_prop_steps'])\n",
    "        \n",
    "        # Train the model\n",
    "        cost_train, _ = sess.run([cost, training_step],\n",
    "                                feed_dict = {\n",
    "                                    X_tensor:X_batch,\n",
    "                                    y_tensor:y_batch,\n",
    "                                    keep_prob:hyperparams['drop_out']\n",
    "                                })\n",
    "        cost_train_ma = cost_train_ma*0.99 + cost_train*0.01\n",
    "        if i%100 == 0:\n",
    "            \n",
    "            # Perform cross validation\n",
    "            result = sess.run([cost, merged_summaries, accuracy],\n",
    "                             feed_dict = {\n",
    "                                 X_tensor: X_val,\n",
    "                                 y_tensor: y_val,\n",
    "                                 keep_prob: 1\n",
    "                             })\n",
    "            cost_val = result[0]\n",
    "            accuracy_val = result[2]\n",
    "            training_step_stats = '{:d} of {:d}. Cost: [{:0.3f}, {:0.3f}], CV Cost: {:0.3f}, CV Accuracy: {:0.3f}'.format(i, hyperparams['epochs'], cost_train, cost_train_ma, cost_val, accuracy_val)\n",
    "            print(training_step_stats)\n",
    "            \n",
    "            # Save model parameters for TensorBoard\n",
    "            summary_str = result[1]\n",
    "            summary_writer.add_summary(summary_str, 1)\n",
    "            summary_writer.flush()\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "    final_accuracy = compute_final_accuracy(X_test, y_test, hyperparams['batch_size'], hyperparams['back_prop_steps'])\n",
    "    print('The final accuracy is {:3f}.'.format(final_accuracy))\n",
    "        \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(hyperparams['batch'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
